{
  "cells": [
    {
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\nComputing ERFs from HCP\n=======================\n\nIn this tutorial we compare different ways of arriving at event related\nfields (ERF) starting from different HCP outputs. We will first reprocess\nthe HCP dat from scratch, then read the preprocessed epochs, finally\nread the ERF files. Subsequently we will compare these outputs.\n\n"
      ]
    },
    {
      "execution_count": null,
      "source": [
        "# Author: Denis A. Enegemann\n# License: BSD 3 clause\n\nimport os.path as op\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mne\nimport hcp\nfrom hcp import io\nimport hcp.preprocessing as preproc\n\nmne.set_log_level('WARNING')\n\n# we assume our data is inside its designated folder under $HOME\nstorage_dir = op.expanduser('~')\nhcp_params = dict(\n    hcp_path=op.join(storage_dir, 'mne-hcp-data', 'HCP'),\n    subject='105923',\n    data_type='task_working_memory')"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We first reprocess the data from scratch\n\nThat is, almost from scratch. We're relying on the ICA solutions and\ndata annotations.\n\nIn order to arrive at the final ERF we need to pool over two runs.\nfor each run we need to read the raw data, all annotations, apply\nthe reference sensor compensation, the ICA, bandpass filter, baseline\ncorrection and decimation (downsampling)\n\n"
      ]
    },
    {
      "execution_count": null,
      "source": [
        "# these values are looked up from the HCP manual\ntmin, tmax = -1.5, 2.5\ndecim = 4\nevent_id = dict(face=1)\nbaseline = (-0.5, 0)\n\n# we first collect events\ntrial_infos = list()\nfor run_index in [0, 1]:\n    hcp_params['run_index'] = run_index\n    trial_info = io.read_trial_info_hcp(**hcp_params)\n    trial_infos.append(trial_info)\n\n\n# trial_info is a dict\n# it contains a 'comments' vector that maps on the columns of 'codes'\n# 'codes is a matrix with its length corresponding to the number of trials\nprint(trial_info['stim']['comments'][:10])  # which column?\nprint(set(trial_info['stim']['codes'][:, 3]))  # check values\n\n# so according to this we need to use the column 7 (index 6)\n# for the time sample and column 4 (index 3) to get the image types\n# with this information we can construct our event vectors\n\nall_events = list()\nfor trial_info in trial_infos:\n    events = np.c_[\n        trial_info['stim']['codes'][:, 6] - 1,  # time sample\n        np.zeros(len(trial_info['stim']['codes'])),\n        trial_info['stim']['codes'][:, 3]  # event codes\n    ].astype(int)\n\n    # for some reason in the HCP data the time events may not always be unique\n    unique_subset = np.nonzero(np.r_[1, np.diff(events[:, 0])])[0]\n    events = events[unique_subset]  # use diff to find first unique events\n\n    all_events.append(events)\n\n# now we can go ahead\nevokeds = list()\nfor run_index, events in zip([0, 1], all_events):\n\n    hcp_params['run_index'] = run_index\n\n    raw = io.read_raw_hcp(**hcp_params)\n    raw.load_data()\n    # apply ref channel correction and drop ref channels\n    preproc.apply_ref_correction(raw)\n\n    annots = io.read_annot_hcp(**hcp_params)\n    # construct MNE annotations\n    bad_seg = (annots['segments']['all']) / raw.info['sfreq']\n    annotations = mne.Annotations(\n        bad_seg[:, 0], (bad_seg[:, 1] - bad_seg[:, 0]),\n        description='bad')\n\n    raw.annotations = annotations\n    raw.info['bads'].extend(annots['channels']['all'])\n    raw.pick_types(meg=True, ref_meg=False)\n\n    # Note: MNE complains on Python 2.7\n    raw.filter(0.50, None, method='iir',\n               iir_params=dict(order=4, ftype='butter'), n_jobs=1)\n    raw.filter(None, 60, method='iir',\n               iir_params=dict(order=4, ftype='butter'), n_jobs=1)\n\n    # read ICA and remove EOG ECG\n    # note that the HCP ICA assumes that bad channels have already been removed\n    ica_mat = hcp.io.read_ica_hcp(**hcp_params)\n\n    # We will select the brain ICs only\n    exclude = [ii for ii in range(annots['ica']['total_ic_number'][0])\n               if ii not in annots['ica']['brain_ic_vs']]\n    preproc.apply_ica_hcp(raw, ica_mat=ica_mat, exclude=exclude)\n\n    # now we can epoch\n    events = np.sort(events, 0)\n    epochs = mne.Epochs(raw, events=events[events[:, 2] == 1],\n                        event_id=event_id, tmin=tmin, tmax=tmax,\n                        reject=None, baseline=baseline, decim=decim,\n                        preload=True)\n\n    evoked = epochs.average()\n    # now we need to add back out channels for comparison across runs.\n    evoked = preproc.interpolate_missing(evoked, **hcp_params)\n    evokeds.append(evoked)\n    del epochs, raw"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Now we can compute the same ERF based on the preprocessed epochs\n\nThese are obtained from the 'tmegpreproc' pipeline.\nThings are pythonized and simplified however, so\n\n"
      ]
    },
    {
      "execution_count": null,
      "source": [
        "evokeds_from_epochs_hcp = list()\n\nfor run_index, events in zip([0, 1], all_events):\n    hcp_params['run_index'] = run_index\n\n    epochs_hcp = io.read_epochs_hcp(**hcp_params)\n    # for some reason in the HCP data the time events may not always be unique\n    unique_subset = np.nonzero(np.r_[1, np.diff(events[:, 0])])[0]\n    evoked = epochs_hcp[unique_subset][events[:, 2] == 1].average()\n\n    del epochs_hcp\n    # These epochs have different channels.\n    # We use a designated function to re-apply the channels and interpolate\n    # them.\n\n    evoked.baseline = baseline\n    evoked.apply_baseline()\n    evoked = preproc.interpolate_missing(evoked, **hcp_params)\n\n    evokeds_from_epochs_hcp.append(evoked)"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Finally we can read the actual official ERF file\n\nThese are obtained from the 'eravg' pipelines.\nWe read the matlab file, MNE-HCP is doing some conversions, and then we\nsearch our condition of interest. Here we're looking at the image as onset.\nand we want the average, not the standard deviation.\n\n"
      ]
    },
    {
      "execution_count": null,
      "source": [
        "evoked_hcp = None\ndel hcp_params['run_index']\nhcp_evokeds = hcp.io.read_evokeds_hcp(onset='stim', **hcp_params)\n\nfor ev in hcp_evokeds:\n    if not ev.comment == 'Wrkmem_LM-TIM-face_BT-diff_MODE-mag':\n        continue\n\n# Once more we add and interpolate missing channels\nevoked_hcp = preproc.interpolate_missing(ev, **hcp_params)"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Time to compare the outputs\n\n\n"
      ]
    },
    {
      "execution_count": null,
      "source": [
        "evoked = mne.combine_evoked(evokeds, weights='equal')\nevoked_from_epochs_hcp = mne.combine_evoked(\n    evokeds_from_epochs_hcp, weights='equal')\n\nfig1, axes = plt.subplots(3, 1, figsize=(12, 8))\n\nevoked.plot(axes=axes[0], show=False)\naxes[0].set_title('MNE-HCP')\n\nevoked_from_epochs_hcp.plot(axes=axes[1], show=False)\naxes[1].set_title('HCP epochs')\n\nevoked_hcp.plot(axes=axes[2], show=False)\naxes[2].set_title('HCP evoked')\nfig1.canvas.draw()\n\nplt.show()\n\n# now some correlations\n\nplt.figure()\nr1 = np.corrcoef(evoked_from_epochs_hcp.data.ravel(),\n                 evoked_hcp.data.ravel())[0][1]\nplt.plot(evoked_from_epochs_hcp.data.ravel()[::10] * 1e15,\n         evoked_hcp.data.ravel()[::10] * 1e15,\n         linestyle='None', marker='o', alpha=0.1,\n         mec='orange', color='orange')\nplt.annotate(\"r=%0.3f\" % r1, xy=(-300, 250))\nplt.ylabel('evoked from HCP epochs')\nplt.xlabel('evoked from HCP evoked')\nplt.show()\n\nplt.figure()\nr1 = np.corrcoef(evoked.data.ravel(), evoked_hcp.data.ravel())[0][1]\nplt.plot(evoked.data.ravel()[::10] * 1e15,\n         evoked_hcp.data.ravel()[::10] * 1e15,\n         linestyle='None', marker='o', alpha=0.1,\n         mec='orange', color='orange')\nplt.annotate(\"r=%0.3f\" % r1, xy=(-300, 250))\nplt.ylabel('evoked from scratch with MNE-HCP')\nplt.xlabel('evoked from HCP evoked file')\nplt.show()"
      ],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.2",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "mimetype": "text/x-python"
    }
  }
}